{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc95cf2b",
   "metadata": {},
   "source": [
    "# Część 1: Podstawy HTML i XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a6603",
   "metadata": {},
   "source": [
    "1. Tagi, atrybuty, i elementy:\n",
    "\n",
    "* Tagi to podstawowe składniki języka HTML i XML. Są one umieszczane w nawiasach ostrokątnych, np. &lt;html>, &lt;body>,  &lt;div>.\n",
    "* Atrybuty dostarczają dodatkowych informacji o tagach. Przykład: w tagu &lt;a href=\"https://example.com\"&gt;, href jest atrybutem definiującym adres URL linku.\n",
    "* Elementy składają się z otwierającego tagu, zawartości i zamykającego tagu. Na przykład, &lt;p>To jest paragraf.&lt;/p>, &lt;a> href=hiperłącze &lt;/a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4be8a",
   "metadata": {},
   "source": [
    "2. Nagłówki w HTML\n",
    "* Nagłówki to elementy HTML używane do organizacji i strukturyzacji treści na stronie internetowej. Są one oznaczane tagami od &lt;h1> do &lt;h6>.\n",
    "* &lt;h1> reprezentuje najważniejszy nagłówek na stronie, zwykle tytuł lub główny punkt strony, a &lt;h6> jest najmniej istotnym nagłówkiem.\n",
    "* Użycie nagłówków pomaga w tworzeniu hierarchii informacji na stronie, co jest ważne zarówno dla użytkowników, jak i dla wyszukiwarek internetowych.\n",
    "Przykład:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d158141",
   "metadata": {},
   "source": [
    "<h1>Tytuł główny strony</h1>\n",
    "<h2>Podsekcja 1</h2>\n",
    "<p>Treść podsekcji 1...</p>\n",
    "<h2>Podsekcja 2</h2>\n",
    "<p>Treść podsekcji 2...</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a8333",
   "metadata": {},
   "source": [
    "3. HTML a XML:\n",
    "* HTML jest używany głównie do tworzenia stron internetowych i jest bardziej elastyczny co do składni.\n",
    "XML służy do przechowywania i przesyłania danych i wymaga ścisłego przestrzegania zasad dobrze sformowanego dokumentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab8a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie przykładowego pliku XML\n",
    "xml_content = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<library>\n",
    "    <book>\n",
    "        <title>Przygody Tomka Sawyera</title>\n",
    "        <author>Mark Twain</author>\n",
    "        <year>1876</year>\n",
    "    </book>\n",
    "    <book>\n",
    "        <title>Pan Tadeusz</title>\n",
    "        <author>Adam Mickiewicz</author>\n",
    "        <year>1834</year>\n",
    "    </book>\n",
    "</library>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093e720",
   "metadata": {},
   "source": [
    "4. Narzędzia do inspekcji strony:\n",
    "\n",
    "Narzędzia deweloperskie w przeglądarkach internetowych (takie jak Chrome, Firefox, Edge) pozwalają na oglądanie struktury HTML, stylów CSS i skryptów JavaScript strony. Można je otworzyć klikając prawym przyciskiem myszy na stronie i wybierając \"Zbadaj\" lub naciskając F12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90449daa",
   "metadata": {},
   "source": [
    "**Ćwiczenie:**\n",
    "\n",
    "Należy utworzyć bardzo prostą stronę internetową zawierającą tytuł, kilka nagłówków i paragrafów. Jeden paragraf ma zawierać hiperłącze do strony MiMUW. Utworzoną stronę proszę zapisać w pliku i podejrzeć w przeglądarce z wykorzystaniem \"Zbadaj\" (zazwyczaj F12).\n",
    "\n",
    "Ewentualnie można skorzystać z poniższej przykładowej implementacji (w zależności od znajomości HTML). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeebe715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie przykładowej strony HTML (do późniejszego przetwarzania przy użyciu BeautifulSoup)\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html> \n",
    "<html>\n",
    "<head>\n",
    "    <title>Page title - Webscraping</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Level 1 header</h1>\n",
    "    <h2>Level 2 header</h2>\n",
    "    <p>A paragraph.</p>\n",
    "    <p>Next paragraph with <a href=\"https://www.mimuw.edu.pl/\">links</a> at the end.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7563ca6-9e60-4e04-a575-012ea01e8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu wpisz rozwiązanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd083725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Po zapisie strony możemy otworzyć ją (domyślnie powinna otworzyć się w przeglądarce internetowej) i obejrzeć w jaki sposób \n",
    "# wygląda i widoczna jest w trybie inspektora (F12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c16f8",
   "metadata": {},
   "source": [
    "# Część 2: Parsowanie HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe75b4",
   "metadata": {},
   "source": [
    "W tej części skupimy się na narzędziach wykorzystywanych do parsowania treści HTML. Bardzo popularnym pakietem jest Beautiful Soup. Jest to pakiet, który służy do parsowania dokumentów HTML i XML. Jest szczególnie użyteczny w web scrapingu, czyli procesie ekstrakcji danych z stron internetowych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cebc84",
   "metadata": {},
   "source": [
    "Napiszmy więc kod, w którym wykorzystamy pakiet BeautifulSoup do analizy naszej strony i wyszukania w niej nagłówków, tytułów oraz paragrafów. Przekonamy się, jak łatwo można przetwarzać HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7f5c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html_file_path = './my_website.html'\n",
    "\n",
    "# Wczytywanie zawartości strony HTML\n",
    "with open(html_file_path, 'r') as file:\n",
    "    html_page = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d71c800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n    <title>Page title - Webscraping</title>\\n</head>\\n<body>\\n    <h1>Level 1 header</h1>\\n    <h2>Level 2 header</h2>\\n    <p>A paragraph.</p>\\n    <p>Next paragraph with <a href=\"https://www.mimuw.edu.pl/\">links</a> at the end.</p>\\n</body>\\n</html>\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ab001",
   "metadata": {},
   "source": [
    "Utworzymy obiekt BeautifulSoup - jako parser wybieramy wbudowany (nie wymagający instalacji/importu dodatkowych pakietów) parser html.\n",
    "Wybór parsera zależny jest od założeń oraz wymagań projektów, jednakże dla tak małych stron html.parser jest wystarczający.\n",
    "W przypadku znacznie większych stron warto rozważyć wykorzystanie lxml, który jest szybszy (wymaga jednak instalacji dodatkowych zależności). Więcej na ten temat można poczytać w dokumentacji:\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00bdd475",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace7a1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<doctype html=\"\">\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "<title>Page title - Webscraping</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>Level 1 header</h1>\n",
       "<h2>Level 2 header</h2>\n",
       "<p>A paragraph.</p>\n",
       "<p>Next paragraph with <a href=\"https://www.mimuw.edu.pl/\">links</a> at the end.</p>\n",
       "</body>\n",
       "</html>\n",
       "</doctype>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3127db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doctype html=\"\">\n",
      " <html lang=\"en\">\n",
      "  <head>\n",
      "   <title>\n",
      "    Page title - Webscraping\n",
      "   </title>\n",
      "  </head>\n",
      "  <body>\n",
      "   <h1>\n",
      "    Level 1 header\n",
      "   </h1>\n",
      "   <h2>\n",
      "    Level 2 header\n",
      "   </h2>\n",
      "   <p>\n",
      "    A paragraph.\n",
      "   </p>\n",
      "   <p>\n",
      "    Next paragraph with\n",
      "    <a href=\"https://www.mimuw.edu.pl/\">\n",
      "     links\n",
      "    </a>\n",
      "    at the end.\n",
      "   </p>\n",
      "  </body>\n",
      " </html>\n",
      "</doctype>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I bardziej czytelnie\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05244ba1",
   "metadata": {},
   "source": [
    "A więc mamy już sparsowaną treść naszej strony - możemy więc przejść do wyszukiwania na niej informacji. W naszym przypadku (choć jest ich niewiele) - możemy pobrać tytuł, nagłówki oraz paragrafy. Spróbujemy również pobrać link do strony MiMUW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eaf87b",
   "metadata": {},
   "source": [
    "Tytuł strony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9a59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyszukiwanie tytułu strony\n",
    "page_title = soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d259de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Page title - Webscraping</title>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "360c88e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page title - Webscraping'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ef099",
   "metadata": {},
   "source": [
    "Nagłówki - h1, h2. Wykorzystamy metody\n",
    "* find() - służy do wyszukiwania pierwszego wystąpienia danego tagu lub tagów spełniających określone kryteria.\n",
    "* find_all() - znajduje wszystkie tagi spełniające te kryteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "560cbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyszukiwanie wszystkich nagłówków h1 i h2\n",
    "headers_h1 = soup.find_all('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b84b8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'string'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mheaders_h1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m \u001b[38;5;66;03m# Nie działa - dlaczego? find_all zwraca listę\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bs4/element.py:2433\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   2432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultSet object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key\n\u001b[1;32m   2435\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'string'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "headers_h1.string # Nie działa - dlaczego? find_all zwraca listę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5952b641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Level 1 header'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_h1[0].string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e903ec",
   "metadata": {},
   "source": [
    "No ale w naszym przypadku mamy przecież 1 element h1 (h2 również), a więc w zupełności wystarczyłoby find. \"W naszym przypadku\", gdyż zazwyczaj nie znamy strony i może ona zawierać setki różnego rodzaju tagów. Bezpieczniej więc skorzystać z find_all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dae25258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dla porównania - find\n",
    "headers_h2 = soup.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8869649e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Level 2 header'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_h2.string # Tutaj już w porządku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ec50b",
   "metadata": {},
   "source": [
    "Wyszukiwanie wszystkich paragrafów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3844e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyszukiwanie wszystkich paragrafów\n",
    "paragraphs = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b41cedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>A paragraph.</p>,\n",
       " <p>Next paragraph with <a href=\"https://www.mimuw.edu.pl/\">links</a> at the end.</p>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e8733d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A paragraph.', None]\n"
     ]
    }
   ],
   "source": [
    "print([p.string for p in paragraphs]) # Pod indeksem 1 mamy None, dlaczego?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9742cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs[1].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30273683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>Next paragraph with <a href=\"https://www.mimuw.edu.pl/\">links</a> at the end.</p>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[1] # Ponieważ w paragrafie znajduje się tag a. Ten z kolei ma atrybut href i jakąś przypisną mu wartość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f8c169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"https://www.mimuw.edu.pl/\">links</a>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tutaj postępujemy w następujący sposób\n",
    "paragraphs[1].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61ba9e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"https://www.mimuw.edu.pl/\">links</a>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lub\n",
    "paragraphs[1].find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "289640d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://www.mimuw.edu.pl/\">links</a>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lub \n",
    "paragraphs[1].find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bd6e833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://www.mimuw.edu.pl/\">links</a>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lub - możliwości jest bardzo dużo\n",
    "# Select - umożliwia wyszukiwanie elementów za pomocą selektorów CSS.\n",
    "paragraphs[1].select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71c92f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mimuw.edu.pl/\n",
      "https://www.mimuw.edu.pl/\n",
      "https://www.mimuw.edu.pl/\n",
      "https://www.mimuw.edu.pl/\n"
     ]
    }
   ],
   "source": [
    "# A następnie odwołujemy się do atrybutu href i mamy hiperłącze do MiMUW :)\n",
    "print(paragraphs[1].a['href'])\n",
    "print(paragraphs[1].find('a')['href'])\n",
    "print(paragraphs[1].find_all('a')[0]['href'])\n",
    "print(paragraphs[1].select('a')[0]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42dc6b7",
   "metadata": {},
   "source": [
    "Zaprezentowanych zostanie teraz kilka innych przydatnych metod z ich wykorzystaniem - na przykładzie naszej strony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1a0907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find() - wyszukuje pierwszy paragraf\n",
    "first_paragraph = soup.find('p').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a8be6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all() - znajduje wszystkie paragrafy\n",
    "all_paragraphs = [p.text for p in soup.find_all('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72a43d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select() - wybiera wszystkie linki (tagi a) w dokumencie\n",
    "all_links = soup.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f0aeb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_one() - wybiera pierwszy tag h1\n",
    "first_h1 = soup.select_one('h1').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89c0a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_parent() - znajduje rodzica pierwszego linku (w tym przypadku paragraf)\n",
    "parent_of_first_link = soup.find('a').find_parent().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbb35405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_next_sibling() - znajduje następujące rodzeństwo po pierwszym nagłówku h1 (w tym przypadku h2)\n",
    "next_sibling_of_h1 = soup.find('h1').find_next_sibling().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec3cbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atrybuty - pobiera atrybut href pierwszego linku\n",
    "first_link_href = soup.find('a')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28d615f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyniki\n",
    "results = {\n",
    "    \"Pierwszy paragraf\": first_paragraph,\n",
    "    \"Wszystkie paragrafy\": all_paragraphs,\n",
    "    \"Wszystkie linki\": [link['href'] for link in all_links],\n",
    "    \"Pierwszy nagłówek h1\": first_h1,\n",
    "    \"Rodzic pierwszego linku\": parent_of_first_link,\n",
    "    \"Następne rodzeństwo po h1\": next_sibling_of_h1,\n",
    "    \"Href pierwszego linku\": first_link_href\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de752611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pierwszy paragraf': 'A paragraph.',\n",
       " 'Wszystkie paragrafy': ['A paragraph.',\n",
       "  'Next paragraph with links at the end.'],\n",
       " 'Wszystkie linki': ['https://www.mimuw.edu.pl/'],\n",
       " 'Pierwszy nagłówek h1': 'Level 1 header',\n",
       " 'Rodzic pierwszego linku': 'Next paragraph with links at the end.',\n",
       " 'Następne rodzeństwo po h1': 'Level 2 header',\n",
       " 'Href pierwszego linku': 'https://www.mimuw.edu.pl/'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51abffb",
   "metadata": {},
   "source": [
    "Select vs find\n",
    "\n",
    "* find i find_all skupiają się na atrybutach i nazwach tagów.\n",
    "* select i select_one zapewniają większą elastyczność dzięki wykorzystaniu selektorów CSS, co pozwala na bardziej złożone zapytania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91accf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = \"\"\"\n",
    "<div>\n",
    "    <p class=\"text\">Pierwszy paragraf</p>\n",
    "    <p class=\"text\">Drugi paragraf</p>\n",
    "    <p class=\"text\" id=\"special\">Trzeci paragraf</p>\n",
    "</div>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82e4aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92d2b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dla tak zdefiniowanej strony mamy:\n",
    "first_paragraph = soup2.find('p')  # Znajduje pierwszy paragraf\n",
    "all_paragraphs = soup2.find_all('p')  # Znajduje wszystkie paragrafy\n",
    "special_paragraph = soup2.select_one('#special')  # Znajduje paragraf z ID 'special'\n",
    "text_paragraphs = soup2.select('p.text')  # Znajduje paragrafy z klasą 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19cbaeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"text\">Pierwszy paragraf</p>,\n",
       " <p class=\"text\">Drugi paragraf</p>,\n",
       " <p class=\"text\" id=\"special\">Trzeci paragraf</p>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c90a5d",
   "metadata": {},
   "source": [
    "Aby osiągnąć za pomocą find znalezienie paragrafów z klasy text musimy wykorzystać argument _class:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b8e3d68",
   "metadata": {},
   "source": [
    "text_paragraphs = soup2.find_all('p', class_='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c3dd912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"text\">Pierwszy paragraf</p>,\n",
       " <p class=\"text\">Drugi paragraf</p>,\n",
       " <p class=\"text\" id=\"special\">Trzeci paragraf</p>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e154734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"text\" id=\"special\">Trzeci paragraf</p>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ewentualnie\n",
    "soup2.find_all('p', attrs={\"class\": \"text\", \"id\": \"special\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6dd19c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"text\" id=\"special\">Trzeci paragraf</p>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warto pokazać tutaj również wykorzystanie id (attrs)\n",
    "soup2.find_all('p', {\"id\": \"special\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a5cae",
   "metadata": {},
   "source": [
    "# Część 3: Pakiet requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c25f9",
   "metadata": {},
   "source": [
    "Wiedza z poprzednich części będzie teraz wykorzystana w praktyce. Zanim przejdziemy do ćwiczenia zapoznamy się z pakietem requests, który umożliwi nam wysyłanie żądań HTTP. Obsługuje on wszystkie popularne metody HTTP, takie jak GET, POST, PUT, DELETE...\n",
    "Inne pakiety tego typu to np. wbudowane urllib oraz http.client. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93393f9e",
   "metadata": {},
   "source": [
    "Aby korzystać z requests, najpierw trzeba zainstalować pakiet. Możemy to zrobić za pomocą pip (systemu zarządzania pakietami w Pythonie):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a1c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "63076652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13651da",
   "metadata": {},
   "source": [
    "POST Request\n",
    "\n",
    "Zapytanie POST służy do wysyłania danych do serwera, na przykład przy przesyłaniu formularza. Niekótre z metod przetestujemy na stronie https://httpbin.org/#/, która umożliwia testowanie zapytań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7349d4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"key1\": \"value1\", \n",
      "    \"key2\": \"value2\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"23\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.32.3\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-673afe8b-075f59ad4dbc87bf3144221a\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"5.173.164.54\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "\n",
    "response = requests.post('https://httpbin.org/post', data=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c781b3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response # Zwraca kod żądania. Więcej można poczytać na przykład tutaj: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9fa9d",
   "metadata": {},
   "source": [
    "Krótka interpretacja: \n",
    "\n",
    "args: pusty obiekt, co oznacza, że do żądania POST nie zostały dołączone żadne parametry URL\n",
    "\n",
    "data: jest puste, co wskazuje, że żadne dane nie zostały wysłane w ciele żądania w formacie innym niż formularz\n",
    "\n",
    "files: jest puste - w żądaniu POST nie wysłano żadnych plików\n",
    "\n",
    "form: pokazuje dane, które zostały przesłane za pomocą żądania POST. W tym przypadku są to klucze i wartości podane w metodzie post.\n",
    "\n",
    "headers: zawiera nagłówki przesłane razem z żądaniem. Są one automatycznie dodawane przez pakiet requests lub serwer. Poniżej opis kilku z nich.\n",
    "\n",
    "* Host: nazwa hosta, do którego skierowane jest żądanie\n",
    "* User-Agent: identyfikuje klienta wykonującego żądanie, tutaj python-requests/2.31.0 oznacza, że wykorzystano pakiet requests w Pythonie.\n",
    "\n",
    "origin: pokazuje adres IP, z którego wysłano żądanie.\n",
    "\n",
    "url: adres URL, na który wysłano żądanie POST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c8287",
   "metadata": {},
   "source": [
    "GET Request\n",
    "\n",
    "Zapytanie GET służy do pobierania danych z określonego zasobu, a więc jest ono najbardziej przydatne podczas webscrapingu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebd34185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://example.com')\n",
    "print(response.text)  # Wyświetla zawartość strony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b8e4db49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056cced",
   "metadata": {},
   "source": [
    "A więc możemy pobrać zawartość strony (jej treść HTML) z wykorzystaniem requests. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a500483",
   "metadata": {},
   "source": [
    "**Ćwiczenie:**\n",
    "Wyszukiwanie poszczególnych elementów na stronie\n",
    "\n",
    "W tym zadaniu proszę (wykorzystując Beautiful Soup) wypisać treść nagłówków, paragrafaów oraz zlokalizować hiperłącza znajdujące się na stronie https://example.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7ddeac5-a44d-46ae-9d06-90d31d67fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "931d6721-3f18-48dd-bf87-bb516410391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1>Example Domain</h1>]\n",
      "[<p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>, <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.iana.org/domains/example']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = soup2.find_all('h1')\n",
    "paragraphs = soup2.find_all('p')\n",
    "hiperLinks = [hiper['href'] for hiper in soup2.find_all('a')]\n",
    "print(headers)\n",
    "print(paragraphs)\n",
    "hiperLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eea9f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu wpisz rozw\n",
    "results = dict(paragraphs=paragraphs, headers=headers, hiperLinks=hiperLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c4b75e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraphs': [<p>This domain is for use in illustrative examples in documents. You may use this\n",
       "      domain in literature without prior coordination or asking for permission.</p>,\n",
       "  <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>],\n",
       " 'headers': [<h1>Example Domain</h1>],\n",
       " 'hiperLinks': ['https://www.iana.org/domains/example']}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a330f",
   "metadata": {},
   "source": [
    "Nagłówki\n",
    "\n",
    "Możemy sami definiować nagłówki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6ae5a5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"key1\": \"value1\", \n",
      "    \"key2\": \"value2\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"23\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Marcin\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-673b02aa-17f583b90f9ce1aa240b6b0b\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"5.173.164.54\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Marcin'}\n",
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "\n",
    "response = requests.post('https://httpbin.org/post', data=payload, headers=headers)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541af7eb",
   "metadata": {},
   "source": [
    "Teraz sami zdefiniowaliśmy nagłówek i widoczne jest to w odpowiedzi. Niekiedy działanie takie może być bardzo przydatne:)\n",
    "\n",
    " \"User-Agent\": \"Marcin\", "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aaa444",
   "metadata": {},
   "source": [
    "Ciasteczka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dd62b234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RequestsCookieJar[]>\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://example.com')\n",
    "print(response.cookies)  # Wyświetla ciasteczka z odpowiedzi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1645afab",
   "metadata": {},
   "source": [
    "# Część 4: FastAPI i requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8723532d",
   "metadata": {},
   "source": [
    "FastAPI to nowoczesny, szybki (wysokowydajny) framework do tworzenia API z Pythonem 3.7+ oparty na standardowych typach Pythona. Jest on używany do tworzenia interfejsów API*\n",
    "\n",
    "*  *https://fastapi.tiangolo.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf819b",
   "metadata": {},
   "source": [
    "Aby rozpocząć, musimy zainstalować FastAPI oraz Uvicorn, który służy jako serwer ASGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc395f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fastapi uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272c23d",
   "metadata": {},
   "source": [
    "Tworzenie prostego endpointu w FastAPI . Oto podstawowy przykład, w którym endpoint HTTP GET zwraca słownik w formacie JSON."
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb0c2803",
   "metadata": {},
   "source": [
    "# Plik main.py\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def read_root():\n",
    "    return {\"Test\": \"API\"}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efa16145",
   "metadata": {},
   "source": [
    "# Polecenie w bashu - main oznacza, że plik main.py zawiera definicję naszej aplikacji i znajduje się w obecnym katalogu\n",
    "# --reload zapewnia restart aplikacji po zmianach w jej kodzie.\n",
    "uvicorn main:app --reload"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e38e3a25",
   "metadata": {},
   "source": [
    "# Podgląd katalogów bazowych oraz konsola po uruchomieniu aplikacji \n",
    "Directory of C:\\Users\\Marcin\\Desktop\\fastapi\n",
    "\n",
    "09.12.2023  16:46    <DIR>          .\n",
    "09.12.2023  16:46    <DIR>          ..\n",
    "09.12.2023  16:45               115 main.py\n",
    "09.12.2023  16:46    <DIR>          __pycache__\n",
    "               1 File(s)            115 bytes\n",
    "               3 Dir(s)  19 312 545 792 bytes free\n",
    "\n",
    "(base) C:\\Users\\Marcin\\Desktop\\fastapi>uvicorn main:app --reload\n",
    "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['C:\\\\Users\\\\Marcin\\\\Desktop\\\\fastapi']\n",
    "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
    "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m15876\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
    "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m8764\u001b[0m]\n",
    "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
    "\u001b[32mINFO\u001b[0m:     Application startup complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c894ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Test\":\"API\"}\n",
      "{'Test': 'API'}\n"
     ]
    }
   ],
   "source": [
    "# Wykonujemy request do naszego API (na przykład możemy wykorzystać PyCharma) - domyślnie pod \n",
    "# http://127.0.0.1:8000 (widoczne powyżej: vicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit))\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:8000/\")\n",
    "response\n",
    "print(response.text)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jako wyjście otrzymujemy {\"Test\": \"API\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd7fb5",
   "metadata": {},
   "source": [
    "# Część 5: Zaawansowany Webscraping (dla zainteresowanych)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b120b1",
   "metadata": {},
   "source": [
    "Strony Dynamiczne:\n",
    "\n",
    "Strony dynamiczne wykorzystują JavaScript do ładowania treści asynchronicznie po załadowaniu głównej struktury strony. Oznacza to, że treści mogą być ładowane i zmieniane bez konieczności przeładowania całej strony."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9de9f",
   "metadata": {},
   "source": [
    "AJAX (Asynchronous JavaScript and XML):\n",
    "\n",
    "AJAX pozwala na wymianę danych z serwerem i aktualizację części strony bez konieczności przeładowania całej strony. Jest to kluczowy element stron dynamicznych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb4950",
   "metadata": {},
   "source": [
    "Uwaga - Beautiful Soup i JavaScript! \n",
    "\n",
    "Beautiful Soup **nie jest** przystosowany do obsługi JavaScript. Potrafi analizować tylko statyczny kod HTML, który otrzymuje.\n",
    "W przypadku stron dynamicznych, które używają JavaScript do ładowania treści, Beautiful Soup nie będzie w stanie uzyskać dostępu do tych dynamicznie generowanych treści.\n",
    "W takich przypadkach lepszym rozwiązaniem jest użycie narzędzi takich jak Selenium, które potrafią obsługiwać JavaScript i dynamiczne treści."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aaaecf",
   "metadata": {},
   "source": [
    "Pakiet Selenium - bardzo często wykorzystywany przy implementacji botów/crawlerów/scraperów, które pracują na stronach dynamicznych. Poniżej kilka cech zgodnie z dokumentacją:\n",
    "\n",
    "* Selenium jest narzędziem automatyzacji przeglądarek, które pozwala na interakcję ze stronami internetowymi tak, jak robiłby to prawdziwy użytkownik.\n",
    "* Selenium może uruchamiać przeglądarkę, wykonywać na niej skrypty JavaScript, kliknąć w elementy strony itp.\n",
    "* Jest to szczególnie przydatne do scrapowania stron, które silnie polegają na JavaScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e7797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following calendar events:\n",
      "                  Name                                              Title  \\\n",
      "8      Talk on seminar     Diversity and evolution of the economic system   \n",
      "0   PhD thesis defence        On Security of Systems Built on Blockchains   \n",
      "11     Talk on seminar             Transducers and straight line programs   \n",
      "5      Talk on seminar               Bregman variation of semimartingales   \n",
      "12     Talk on seminar  Normalized solutions to elliptic problems - ne...   \n",
      "\n",
      "                Date             Speaking  \\\n",
      "8   2024-11-20 14:15        Elżbieta Pliś   \n",
      "0   2024-11-22 12:00       Tomasz Lizurej   \n",
      "11  2024-11-20 14:15    Mikołaj Bojańczyk   \n",
      "5   2024-11-21 12:15        Dominik Kutek   \n",
      "12  2024-11-21 12:30  Bartosz Bieganowski   \n",
      "\n",
      "                                                 Link  Room  \n",
      "8   https://mimuw.edu.pl/en/seminars/talk/diversit...  5070  \n",
      "0                                             no link  2180  \n",
      "11  https://mimuw.edu.pl/en/seminars/talk/transduc...  5440  \n",
      "5   https://mimuw.edu.pl/en/seminars/talk/bregman-...  3160  \n",
      "12  https://mimuw.edu.pl/en/seminars/talk/normaliz...  5070  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.mimuw.edu.pl/en/\")\n",
    "driver.implicitly_wait(0.5)\n",
    "# Można teraz dokonywać interakcji ze stroną, np. klikając w przyciski, wypełniając formularze itp.\n",
    "# Źródło strony\n",
    "#print(driver.page_source)\n",
    "def extract_from_doctorate(html):\n",
    "    date = \"no date\"\n",
    "    Title= \"no title\"\n",
    "    link= \"no link\"\n",
    "    room= \"no room\"\n",
    "    speaker= \"no speaker\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tooltip = soup.find('div', class_='calendar-tooltip')\n",
    "    event_elements = tooltip.find_all('tr')\n",
    "    header = tooltip.find('div', class_='calendar-tooltip-header').text\n",
    "    if not event_elements:\n",
    "        return \"No events found.\"\n",
    "    for info in event_elements:\n",
    "        label = info.find('td', class_='calendar-tooltip-label')\n",
    "        text = info.find('td', class_='calendar-tooltip-text')\n",
    "        # print(label.text, text.text)\n",
    "        if label and text:\n",
    "            if(label.text == \"From\"):\n",
    "                date = text.text\n",
    "            elif(label.text == \"Title\" or label.text == \"Thesis\"):\n",
    "                Title = text.text\n",
    "                linked = text.find('a')\n",
    "                if linked:\n",
    "                    link = linked['href']\n",
    "                    if not link.startswith('https'):\n",
    "                        link = 'https://mimuw.edu.pl' + link\n",
    "            elif(label.text == \"Link\"):\n",
    "                link = text.find('a')['href']\n",
    "            elif(label.text == \"Room\"):\n",
    "                room = text.text\n",
    "            elif(label.text == \"Speaker(s)\" or label.text == \"Phd Student\"):\n",
    "                speaker = text.text\n",
    "    return {'Name': header, 'Title': Title,'Date': date, 'Speaking': speaker, 'Link': link, 'Room': room}                \n",
    "\n",
    "\n",
    "\n",
    "calendar_events = driver.find_elements(By.CLASS_NAME, 'calendar-event')\n",
    "#print(calendar_events)\n",
    "if calendar_events:\n",
    "    events_list = []\n",
    "    print(\"Found the following calendar events:\")\n",
    "    for index, event in enumerate(calendar_events, start=1):\n",
    "        full = event.get_attribute(\"outerHTML\")\n",
    "        row = extract_from_doctorate(full)\n",
    "        events_list.append(row)\n",
    "    df = pd.DataFrame(events_list)\n",
    "\n",
    "    print(df.sample(5)) \n",
    "    df.to_csv('calendar_events.csv', index=False)\n",
    "else:\n",
    "    print(\"No calendar events found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3aba7",
   "metadata": {},
   "source": [
    "CAPTCHA i Bot Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe5e68",
   "metadata": {},
   "source": [
    "* UWAGA: Zawsze przestrzegaj zasad i warunków korzystania ze strony!\n",
    "\n",
    "* Scrapowanie możemy rozpocząć tylko wtedy, gdy mamy zgodę właściciela treści, administratora serwera, strony lub strona zezwala na automatyzację zapytań.\n",
    "\n",
    "* Strony internetowe często używają CAPTCHA i mechanizmów wykrywania botów, aby zapobiec automatycznemu scrapowaniu i innym formom nadużyć.\n",
    "\n",
    "* Automatyczne obejście CAPTCHA i mechanizmów wykrywania botów może naruszać warunki korzystania z serwisu i być nieetyczne.\n",
    "\n",
    "* Do dużych projektów programistycznych można wykorzystać również pakiet scrapy, który z wykorzystaniem scrapy.Spider pozwala na budowę zaawansowanych scraperów."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
